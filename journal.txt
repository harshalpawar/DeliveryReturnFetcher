New problems
r.jina.ai/hm.com finds Free shipping above ₹1999
but it's not in my scraped data
The links themselves are bad - this means jina search is bad! I need to try with brave.

For nike - jina finds ₹ 1 250.00 for all orders but gemini doesn't
This is a known issue. I need to prompt better and write clean.py

Scraped data have to go in a different data file because they fuck up my logs
It's 100% jina search issue, basically the keywords that work for hm don't work for nike and vice versa
I checked and brave works well for both nike and hm

Another problem is that nike declares a different delivery time on their policy page and product pages so this will have to be handled by the prompt. Just return the standard response.

The prompt will also need to handle memberships. Ignore delivery and return info under memberships.

Pages have a lot of noise and sites keep thinking I'm from the US

Try this idea - clean pages, extract relevant information and get final response all through Gemini by making separate API calls
Another idea - make a separate call to Gemini and have it verify the output. Also try the thinking model.
Another idea - if search fails, retry with less specific keywords instead of skipping.
Another idea - check how Jina is modifying the URLs. Store and hash the API responses.

TODO-
[x] Clean up the code
[ ] Switch to Brave Search (Critical for search accuracy).
[x] Separate Scraped Data (Quick fix to clean up logs).
[x] Optimise Gemini Prompt
[x] Include Home Page and Product Page (Some brands have the info here).
[x] Run the Pipeline for 5 Brands (Deliver something tangible).
[x] Do the 20 brand test (test on a diverse dataset)
[x] Clean up the code
[x] Excalidraw with challenges
[ ] Get a Gemini/other API key

[x] Look at the current feedback
[x] Look at the complete feedback
[x] Improve prompt
[x] Optimise Gemini config (Improves extraction accuracy).
[x] Check out structured output and any other useful options in Gemini docs
[ ] Learn statistics for calcing the accuracy/confidence
[ ] Design a system for calcing accuracy/confidence
[ ] Explore finetuning by giving it input/output examples

Another idea - since many sites link to their policy pages on the home page, scrape the home page and check if there is a policy page linked
Jina adds this - ?srsltid= - to every URL. Remove it.
keyword searching within the text combined with positional biasing is a more robust and safer strategy than URL filtering

Biggest improvements will come from - 
[ ] Using a better search engine
[x] Improving my prompt to cover as many edge cases as possible
[x] Using a reasoning LLM
[x] Crawling and find the policy pages (incase search fails)
[x] Remove URL tags
[x] Cache the scraped response

gemini split step

new idea - gemini scraped pages + human-collected info - just verify

[x] verify the code
[x] Optimise Jina config
[x] fixed urls in input
[x] use playwright for fetchpolicypages
[x] added url normalisation
[x] fix the program
    [x] URL matching problem with https:// and www
    [x] new problem with searching in fetchPolicyPages
[x] Run tests
[x] Implement fallback to scraping with Playwright if Jina fails
[x] Add basic url filtering with llm
[x] Add a use Playwright only filter in scrape.py
[ ] Write a script to verify the cache
[x] remove fabindia from cache
[x] swap to playwright_stealth
[x] verify code and clean up before rerunning tests
[ ] fix the networkidle error
[x] Rerun tests
[x] Update sheet with new results

[x] implement verification idea
    [x] Import spreadsheet into codebase
    [x] Write a new function for verification with Gemini
    [x] Write a new prompt
    [x] run the tests

[ ] Write clean.py (split scraped data into chunks and rank them by relevance)
[ ] Test trying to split to inference step
[ ] Have Gemini verify it's own output in a seperate API request
[ ] fix the test directory
[ ] documentation, comments and make the code push-ready.

MASSIVE COMMIT! optimised jina config, added url normalisation for caching, url deduplication, playwright for fetchpolicypages, ran tests on all brands, imported human data, wrote new prompt for verification, corrected input urls

implemented verification fully, added playwright+markdownify as a fallback, url filtering with gemini

Problem in search - if 1 part fails, we skip even if other parts found urls to process

I've encountered new problems with Jina Reader, sometimes it fails to scrape the page when my playwright solution does and sometimes it cleans useful parts of the page.

Program runs fine - I'm running tests on all brands

Problem sites - prose stories, rareism, uniqlo, summer somewhere, forever 21, fabindia

Potential issue is that the cached data might be bad.

URL filtering is bad. Info loss for H&M. Disabled.

redo Jack & Jones, Littlebox
Prose stories cache delete and redo
problems - fabindia, forever21

Final response for Fabindia: Apologies, but I am unable to extract the delivery and return policy from the provided web page contents. The content for each URL you provided is just the message "Please enable JavaScript to continue using this application.", and does not contain the actual text of the Fabindia website's policy pages.

To proceed, please provide the actual text content from the following Fabindia pages:

*   **Shipping & Delivery page:**  `https://www.fabindia.com/shipping-delivery`
*   **Return & Exchange page:** `https://www.fabindia.com/return-exchange`

Once you provide the text content from these pages, I will be able to follow the steps and extract the delivery and return policy in the requested format.

Forever21 is problematic still https://content.abfrl.in/faqdetail?uid=How%20do%20I%20know%20my%20order%20is%20confirmed?

Using my new prompt is giving good results
[x] remove prosestories from cache
[ ] refine prompt
[x] check code
[x] fix playwright error (fallback with requests)
[ ] fix url filter

return days matters
delivery days doesn't matter as much

free delivery flag should not be raised for deepika arora ignore cod

write new prompt
get results for verification & extraction
save urls for each brand